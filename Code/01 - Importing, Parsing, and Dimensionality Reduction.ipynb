{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Organization of this File:\n",
    "\n",
    "## 0. Package Loading\n",
    "## I. Importing the data\n",
    "## II. Create General Conversion Table for \"Special Variables\"\n",
    "### III. Feature Engineering: Basic\n",
    "### IV. Feature Engineering: Running SVD on some particular features..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Package Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_columns = 500\n",
    "\n",
    "from sklearn.datasets import dump_svmlight_file\n",
    "from scipy.sparse import csr_matrix, find\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "from scipy.sparse.linalg import svds\n",
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Importing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. Getting the Movie Metadata from hetrec2011 data  (MovieLens augmented w/ IMDB + RottenTomatoes)\n",
    "\n",
    "- Inputs: hetrec2011-movielens-2k-v2 files\n",
    "- Outputs: dictionary_of_hetrec_dataframes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hetrec2011_dat_files = !ls ../Resources/Data/Raw/hetrec2011-movielens-2k-v2/\n",
    "hetrec2011_dat_files.remove(\"readme.txt\")\n",
    "hetrec2011_dat_files_pwd = []\n",
    "hetrec2011_table_names = []\n",
    "for i in range(len(hetrec2011_dat_files)):\n",
    "    # This creates a new item in the hetrec2011_dat_files_pwd list which has the full directory path\n",
    "    hetrec2011_dat_files_pwd.append(\"../Resources/Data/Raw/hetrec2011-movielens-2k-v2/\" + hetrec2011_dat_files[i])\n",
    "    # This distills out just the names of the taples (without \"dat\" at the end)\n",
    "    hetrec2011_table_names.append(hetrec2011_dat_files[i][:-4])\n",
    "\n",
    "# hetrec2011_dat_files_pwd\n",
    "# hetrec2011_table_names\n",
    "# hetrec2011_table_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary_of_hetrec_dataframes = {}\n",
    "n = len(hetrec2011_dat_files_pwd)\n",
    "\n",
    "for i in range(n):\n",
    "    table_name = hetrec2011_table_names[i]\n",
    "    table_path = hetrec2011_dat_files_pwd[i]\n",
    "    dictionary_of_hetrec_dataframes[table_name] = pd.read_table(table_path)\n",
    "\n",
    "dictionary_of_hetrec_dataframes[\"movies\"].rename(columns={'id':'movieID'}, inplace=True)\n",
    "dictionary_of_hetrec_dataframes[\"tags\"].rename(columns={'id':'tagID'}, inplace=True)\n",
    "dictionary_of_hetrec_dataframes[\"tags\"].rename(columns={'value':'tag_value'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['movie_actors',\n",
       " 'user_ratedmovies-timestamps',\n",
       " 'movie_locations',\n",
       " 'movie_directors',\n",
       " 'tags',\n",
       " 'user_taggedmovies',\n",
       " 'user_taggedmovies-timestamps',\n",
       " 'movies',\n",
       " 'movie_genres',\n",
       " 'movie_countries',\n",
       " 'movie_tags',\n",
       " 'user_ratedmovies']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary_of_hetrec_dataframes.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = \"../Resources/Data/Raw/hetrec2011-movielens-2k-v2/movies.dat\"\n",
    "movies_df = pd.read_table(path)\n",
    "\n",
    "list_of_movieIDs = movies_df[\"id\"]\n",
    "list_of_movieTitles = movies_df[\"title\"]\n",
    "list_of_urls = movies_df[\"imdbPictureURL\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### B. Getting the User Ratings data from ML-Latest (Movie Lens Latest)\n",
    "\n",
    "- Inputs: ml-latest\n",
    "- Outputs: user_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "movie_lens22M_csv_files = !ls ../Resources/Data/Raw/ml-latest/\n",
    "movie_lens22M_csv_files.remove(\"README.txt\")\n",
    "movie_lens22M_csv_files_pwd = []\n",
    "movie_lens22M_table_names = []\n",
    "for i in range(len(movie_lens22M_csv_files)):\n",
    "    # This creates a new item in the movie_lens22M_csv_files_pwd list which has the full directory path\n",
    "    movie_lens22M_csv_files_pwd.append(\"../Resources/Data/Raw/ml-latest/\" + movie_lens22M_csv_files[i])\n",
    "    # This distills out just the names of the taples (without \"csv\" at the end)\n",
    "    movie_lens22M_table_names.append(movie_lens22M_csv_files[i][:-4])\n",
    "\n",
    "# movie_lens22M_table_names\n",
    "# movie_lens22M_csv_files_pwd\n",
    "\n",
    "dictionary_of_movie_lens22M_dataframes = {}\n",
    "n = len(movie_lens22M_csv_files_pwd)\n",
    "\n",
    "for i in range(n):\n",
    "    table_name = movie_lens22M_table_names[i]\n",
    "    table_path = movie_lens22M_csv_files_pwd[i]\n",
    "    dictionary_of_movie_lens22M_dataframes[table_name] = pd.read_csv(table_path)\n",
    "\n",
    "# Making the movieID & imdbID & userid & tmdbId uniform...\n",
    "dictionary_of_movie_lens22M_dataframes[\"links\"].rename(columns={'movieId':'movieID'}, inplace=True)\n",
    "dictionary_of_movie_lens22M_dataframes[\"ratings\"].rename(columns={'movieId':'movieID'}, inplace=True)\n",
    "dictionary_of_movie_lens22M_dataframes[\"movies\"].rename(columns={'movieId':'movieID'}, inplace=True)\n",
    "dictionary_of_movie_lens22M_dataframes[\"links\"].rename(columns={'imdbId':'imdbID'}, inplace=True)\n",
    "dictionary_of_movie_lens22M_dataframes[\"ratings\"].rename(columns={'userId':'userID'}, inplace=True)\n",
    "dictionary_of_movie_lens22M_dataframes[\"ratings\"].rename(columns={'tmdbId':'tmdbID'}, inplace=True)\n",
    "\n",
    "# Removing the genres from the movies since I won't have the extra \n",
    "# metadata on the movies anyway, I don't want to get distracted by their presence.\n",
    "# del dictionary_of_movie_lens22M_dataframes[\"movies\"][\"genres\"]\n",
    "\n",
    "user_ratings = dictionary_of_movie_lens22M_dataframes[\"ratings\"]\n",
    "del user_ratings[\"timestamp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>movieID</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>122</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>172</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1221</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1441</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1609</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userID  movieID  rating\n",
       "0       1      122     2.0\n",
       "1       1      172     1.0\n",
       "2       1     1221     5.0\n",
       "3       1     1441     4.0\n",
       "4       1     1609     3.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Creating the different y labels:\n",
    "\n",
    "# label_type = how the \"y\" is coded in.\n",
    "# # binned_5 = ratings of 1,2,3,4,5\n",
    "# # binned_3 = ratings of <3, 3, >3\n",
    "# # binary = ratings of <3 or <= 3\n",
    "# # original_9 = ratings of 0.5,1,1.5,2,2.5,3,3.5,4,4.5,5) get converted to (1,2,3..10) b/c otherwise the classificaiton functions freak out\n",
    "\n",
    "# This will bin half ratings like 0.5 and 1 into the same \"1\" group. Likewise 1.5 and 2 get binned into 2...\n",
    "user_ratings.loc[(user_ratings['rating'] <= 1), 'binned_5'] = 1\n",
    "user_ratings.loc[(user_ratings['rating'] > 1) & (user_ratings['rating'] <= 2), 'binned_5'] = 2\n",
    "user_ratings.loc[(user_ratings['rating'] > 2) & (user_ratings['rating'] <= 3), 'binned_5'] = 3\n",
    "user_ratings.loc[(user_ratings['rating'] > 3) & (user_ratings['rating'] <= 4), 'binned_5'] = 4\n",
    "user_ratings.loc[(user_ratings['rating'] > 4), 'binned_5'] = 5\n",
    "\n",
    "# Binning into groups of 3\n",
    "user_ratings.loc[(user_ratings['rating'] < 3), 'binned_3'] = 1\n",
    "user_ratings.loc[(user_ratings['rating'] == 3), 'binned_3'] = 3\n",
    "user_ratings.loc[(user_ratings['rating'] > 3), 'binned_3'] = 5\n",
    "\n",
    "# Binning into groups of 2\n",
    "user_ratings.loc[(user_ratings['rating'] < 3), 'binary'] = 0\n",
    "user_ratings.loc[(user_ratings['rating'] >= 3), 'binary'] = 1\n",
    "\n",
    "# Renaming the original 9 groupings\n",
    "# user_ratings.rename(columns={'rating':'original_9'}, inplace=True)\n",
    "# there's probably a faster way of doing this...\n",
    "user_ratings.loc[(user_ratings['rating'] == 0.5), 'original_9'] = 1\n",
    "user_ratings.loc[(user_ratings['rating'] == 1), 'original_9'] = 2\n",
    "user_ratings.loc[(user_ratings['rating'] == 1.5), 'original_9'] = 3\n",
    "user_ratings.loc[(user_ratings['rating'] == 2), 'original_9'] = 4\n",
    "user_ratings.loc[(user_ratings['rating'] == 2.5), 'original_9'] = 5\n",
    "user_ratings.loc[(user_ratings['rating'] == 3), 'original_9'] = 6\n",
    "user_ratings.loc[(user_ratings['rating'] == 3.5), 'original_9'] = 7\n",
    "user_ratings.loc[(user_ratings['rating'] == 4), 'original_9'] = 8\n",
    "user_ratings.loc[(user_ratings['rating'] == 4.5), 'original_9'] = 9\n",
    "user_ratings.loc[(user_ratings['rating'] == 5), 'original_9'] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>movieID</th>\n",
       "      <th>rating</th>\n",
       "      <th>binned_5</th>\n",
       "      <th>binned_3</th>\n",
       "      <th>binary</th>\n",
       "      <th>original_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>122</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>172</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1221</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1441</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1609</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userID  movieID  rating  binned_5  binned_3  binary  original_9\n",
       "0       1      122     2.0       2.0       1.0     0.0         4.0\n",
       "1       1      172     1.0       1.0       1.0     0.0         2.0\n",
       "2       1     1221     5.0       5.0       5.0     1.0        10.0\n",
       "3       1     1441     4.0       4.0       5.0     1.0         8.0\n",
       "4       1     1609     3.0       3.0       3.0     1.0         6.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1168997.,        0.,  2051872.,        0.,        0.,  6157082.,\n",
       "               0.,  9461105.,        0.,  5565040.]),\n",
       " array([ 1. ,  1.4,  1.8,  2.2,  2.6,  3. ,  3.4,  3.8,  4.2,  4.6,  5. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEGCAYAAACevtWaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x110b869d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#plt.hist(user_ratings[\"binned_5\"]),bins=k_clusters)\n",
    "plt.hist(user_ratings[\"binned_5\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_ratings.to_pickle(\"../Resources/Pickles/user_ratings.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## II. Create General Conversion Table for \"Special Variables\"\n",
    "\n",
    "\"lookup_table_movies\" is a datframe with conversions across these things:\n",
    "- movieID\n",
    "- movie Title\n",
    "- imdbID\n",
    "- rtID - rotten tomatoes id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lookup_table_movies = dictionary_of_hetrec_dataframes[\"movies\"][[\"movieID\",\"title\",\"imdbID\",\"rtID\"]]\n",
    "lookup_table_movies.to_pickle(\"../Resources/Pickles/lookup_table_movies.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. Feature Engineering: Basic:\n",
    "\n",
    "- Inputs: dictionary_of_hetrec_dataframes\n",
    "- Outputs: below\n",
    "\n",
    "imdb_and_rt_ratings_feature = ....\n",
    "misc_movie_features = (year)\n",
    "\n",
    "##### \"Big\" Features which will require dumificaiton + dimensionality reduction\n",
    "genre_feature_raw (movieID & genre) --------> genre_feature_dummied -----> genre_feature_dim_reduced\n",
    "director_feature_raw (movieID & directorID)-> director_feature_dummied --> director_feature_dim_reduced\n",
    "tags_feature_raw (movieID & tagID)----------> tags_feature_dummied ------> tags_feature_dim_reduced\n",
    "actor_feature_raw (movieID & actorID)-------> actors_feature_dummied --> actors_feature_dim_reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. Creating the raw features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imdb_and_rt_ratings_feature = dictionary_of_hetrec_dataframes[\"movies\"][[\"movieID\",\"rtAllCriticsRating\",\"rtAllCriticsNumReviews\",\"rtAllCriticsNumFresh\",\"rtAllCriticsNumRotten\",\"rtAllCriticsScore\",\"rtTopCriticsRating\",\"rtTopCriticsNumReviews\",\"rtTopCriticsNumFresh\",\"rtTopCriticsNumRotten\",\"rtTopCriticsScore\",\"rtAudienceRating\",\"rtAudienceNumRatings\",\"rtAudienceScore\"]]\n",
    "misc_movie_features = dictionary_of_hetrec_dataframes[\"movies\"][[\"movieID\",\"year\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "movieID                    int64\n",
       "rtAllCriticsRating        object\n",
       "rtAllCriticsNumReviews    object\n",
       "rtAllCriticsNumFresh      object\n",
       "rtAllCriticsNumRotten     object\n",
       "rtAllCriticsScore         object\n",
       "rtTopCriticsRating        object\n",
       "rtTopCriticsNumReviews    object\n",
       "rtTopCriticsNumFresh      object\n",
       "rtTopCriticsNumRotten     object\n",
       "rtTopCriticsScore         object\n",
       "rtAudienceRating          object\n",
       "rtAudienceNumRatings      object\n",
       "rtAudienceScore           object\n",
       "dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_and_rt_ratings_feature.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an object: rtAllCriticsRating\n",
      "This should no longer be an object:  <class 'pandas.core.series.Series'>\n",
      "This is an object: rtAllCriticsNumReviews\n",
      "This should no longer be an object:  <class 'pandas.core.series.Series'>\n",
      "This is an object: rtAllCriticsNumFresh\n",
      "This should no longer be an object:  <class 'pandas.core.series.Series'>\n",
      "This is an object: rtAllCriticsNumRotten\n",
      "This should no longer be an object:  <class 'pandas.core.series.Series'>\n",
      "This is an object: rtAllCriticsScore\n",
      "This should no longer be an object:  <class 'pandas.core.series.Series'>\n",
      "This is an object: rtTopCriticsRating\n",
      "This should no longer be an object:  <class 'pandas.core.series.Series'>\n",
      "This is an object: rtTopCriticsNumReviews\n",
      "This should no longer be an object:  <class 'pandas.core.series.Series'>\n",
      "This is an object: rtTopCriticsNumFresh\n",
      "This should no longer be an object:  <class 'pandas.core.series.Series'>\n",
      "This is an object: rtTopCriticsNumRotten\n",
      "This should no longer be an object:  <class 'pandas.core.series.Series'>\n",
      "This is an object: rtTopCriticsScore\n",
      "This should no longer be an object:  <class 'pandas.core.series.Series'>\n",
      "This is an object: rtAudienceRating\n",
      "This should no longer be an object:  <class 'pandas.core.series.Series'>\n",
      "This is an object: rtAudienceNumRatings\n",
      "This should no longer be an object:  <class 'pandas.core.series.Series'>\n",
      "This is an object: rtAudienceScore\n",
      "This should no longer be an object:  <class 'pandas.core.series.Series'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "for column in imdb_and_rt_ratings_feature.columns:\n",
    "    if imdb_and_rt_ratings_feature[column].dtype == object:\n",
    "        print \"This is an object:\", column\n",
    "        imdb_and_rt_ratings_feature[column] = pd.to_numeric(imdb_and_rt_ratings_feature[column], errors= \"coerce\")\n",
    "        print \"This should no longer be an object: \", type(imdb_and_rt_ratings_feature[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imdb_and_rt_ratings_feature.to_pickle(\"../Resources/Pickles/imdb_and_rt_ratings_feature.pickle\")\n",
    "misc_movie_features.to_pickle(\"../Resources/Pickles/misc_movie_features.pickle\")\n",
    "\n",
    "genre_1_raw = dictionary_of_hetrec_dataframes[\"movie_genres\"][[\"movieID\",\"genre\"]]             # 20 genres\n",
    "directors_1_raw = dictionary_of_hetrec_dataframes[\"movie_directors\"][[\"movieID\",\"directorID\"]] # 10k directors in total\n",
    "tags_1_raw = pd.merge(dictionary_of_hetrec_dataframes[\"movie_tags\"],dictionary_of_hetrec_dataframes[\"tags\"], on= \"tagID\")[[\"movieID\",\"tag_value\"]]  # 5k tags\n",
    "actors_1_raw = dictionary_of_hetrec_dataframes[\"movie_actors\"][[\"movieID\",\"actorID\"]]          # 200 k actors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. Creating dummies as necessary - [Caveat - Potentially time intensive process, so once complete, pickling and ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Before diving in... an alternate way to get dummies  ... a special thing needs to be create for the actors_feature_dummied because of it's size...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This kills the kernal, need to try an alternate approuch !!!!!\n",
    "# temp_actors_a = pd.get_dummies(actors_1_original, columns =[\"actorID\"])\n",
    "# temp_actors_b = temp_actors_a.groupby(\"actorID\", as_index = False)\n",
    "# actors_2_as_dummies = temp_actors_b.sum()\n",
    "\n",
    "# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "\n",
    "# I tried to create a funciton from here: http://stackoverflow.com/questions/34407953/how-to-create-dummy-variable-and-then-aggregate-using-scikit-learn\n",
    "#... but it didn't work\n",
    "\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# mapping = {tagID: movieID for movieID, tagID in enumerate(tags_1_original[\"tagID\"].unique())}\n",
    "# reverse_mapping = {movieID: tagID for tagID, movieID in mapping.iteritems()}\n",
    "\n",
    "# hot = OneHotEncoder()\n",
    "# h = hot.fit_transform(tags_1_original[\"tagID\"].map(mapping).values.reshape(len(tags_1_original), 1))\n",
    "\n",
    "# h.toarray().shape\n",
    "\n",
    "# # [reverse_mapping[n] for n in reverse_mapping.keys()]\n",
    "# from scipy.sparse import hstack, lil_matrix\n",
    "\n",
    "# id_vals = lil_matrix(tags_1_original[\"movieID\"].values.reshape(len(tags_1_original), 1))\n",
    "# h_dense = hstack([id_vals, h.tolil()])\n",
    "# h_dense.toarray().shape\n",
    "\n",
    "# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "# Then I tried a different approuch from here: # https://github.com/coreylynch/sklearn-transform/blob/master/dump_cat_df_to_svm_light.py\n",
    "# Which also ended up having problems...\n",
    "\n",
    "# def map_cat_to_index(X,column):\n",
    "# \tnames = set(X[column])\n",
    "# \tindex = range(len(set(X[column])))\n",
    "# \tcat_to_index = dict(zip(names,index))\n",
    "# \treturn np.array([cat_to_index[i] for i in X[column]]) \n",
    "\n",
    "# def convert_to_one_hot(X,cat_columns):\n",
    "# \tfor column in cat_columns:\n",
    "# \t\tX[column] = map_cat_to_index(X,column)\n",
    "\n",
    "# \tprev_max = max(X[cat_columns[0]])\n",
    "# \tfor column in cat_columns[1:]:\n",
    "# \t\tcol_max = max(X[column])\n",
    "# \t\tX[column] = np.array([val+prev_max for val in X[column]])\n",
    "# \t\tprev_max += col_max\n",
    "# \treturn X\n",
    "\n",
    "# def categorical_df_to_csr(X, cat_columns, num_columns=None):\n",
    "# \t\"\"\"Converts a categorical DataFrame X to a sparse CSR matrix.\n",
    "\t\n",
    "# \tThis takes a pandas DataFrame with categorical features and converts category\n",
    "# \tvalues to a sparse one-hot representation.\n",
    "\t\n",
    "# \tParameters\n",
    "# \t----------\n",
    "# \tX : pandas DataFrame, shape = [n_samples, n_features]\n",
    "# \t    Training vectors, where n_samples is the number of samples and\n",
    "# \t    n_features is the number of features.\n",
    "# \ty : array-like, shape = [n_samples]\n",
    "# \t    Target values.\n",
    "# \tcat_columns: array-like\n",
    "# \t\tList of categorical columns\n",
    "# \tnum_columns: array-like, optional\n",
    "# \t\tList of numerical columns\n",
    "# \tReturns\n",
    "# \t-------\n",
    "# \tsparse_X : sparse CSR matrix\n",
    "# \t\tSparse vectorized version of categorical DataFrame\n",
    "# \t\"\"\"\n",
    "# \tX = convert_to_one_hot(X,cat_columns)\n",
    "# \tcat_col_indexes = np.concatenate([X[i] for i in X[cat_columns]])\n",
    "# \tif len(cat_columns)>1:\n",
    "# \t\tcat_rows = np.concatenate([X.index for i in range(len(cat_columns))])\n",
    "# \telse:\n",
    "# \t\tcat_rows = X.index\n",
    "# \tmax_col_value = X.max()[cat_columns[-1]]\n",
    "# \tdata = np.ones(len(cat_col_indexes))\n",
    "# \tif num_columns is None:\n",
    "# \t\tsparse_X = csr_matrix((data,(cat_rows,cat_col_indexes)))\n",
    "# \telse:\n",
    "# \t\tnum_col_indexes = np.array([max_col_value+i for i in range(len(X.index)) for j in range(len(num_columns))])\n",
    "# \t\tif len(num_columns)>1:\n",
    "# \t\t\t#num_rows = [X.index for i in range(len(num_columns))]\n",
    "# \t\t\tnum_rows = np.array([i for i in X.index for j in range(len(num_columns))])\n",
    "# \t\t\tall_col_indexes = np.concatenate((cat_col_indexes, num_col_indexes))\t\n",
    "# \t\telse:\n",
    "# \t\t\tnum_rows = X.index\n",
    "# \t\t\tall_col_indexes = np.concatenate((cat_col_indexes, num_col_indexes[0]))\n",
    "# \t\trows = np.concatenate((cat_rows,num_rows))\n",
    "# \t\tnum_data = np.concatenate([X[i] for i in X[num_columns]])\n",
    "# \t\tdata = np.concatenate((data, num_data))\n",
    "# \t\tsparse_X = csr_matrix((data,(rows,all_col_indexes)))\n",
    "# \treturn sparse_X\n",
    "\n",
    "# actors_2_as_dummies = categorical_df_to_csr(actors_1_original, cat_columns= [\"actorID\"], num_columns = [\"movieID\"])\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# ValueError                                Traceback (most recent call last)\n",
    "# <ipython-input-34-8ed5f1bab115> in <module>()\n",
    "# ----> 1 actors_2_as_dummies = categorical_df_to_csr(actors_1_original, cat_columns= [\"actorID\"], num_columns = [\"movieID\"])\n",
    "\n",
    "# <ipython-input-33-a3163c1cf8ef> in categorical_df_to_csr(X, cat_columns, num_columns)\n",
    "#      39                 else:\n",
    "#      40                         num_rows = X.index\n",
    "# ---> 41                         all_col_indexes = np.concatenate((cat_col_indexes, num_col_indexes[0]))\n",
    "#      42                 rows = np.concatenate((cat_rows,num_rows))\n",
    "#      43                 num_data = np.concatenate([X[i] for i in X[num_columns]])\n",
    "\n",
    "# ValueError: all the input arrays must have same number of dimensions\n",
    "    \n",
    "# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "\n",
    "# Something that actually works... or not...\n",
    "# def create_sparse_dummy_data_frame(DataFrame):\n",
    "#     \"\"\"When pd.get_dummies fails you... try this.\n",
    "#     DataFrame should have 2 columns: the person & the categorical thing which needs to be dummied.\n",
    "    \n",
    "#     References: See Chen/Affatus back and forth here:\n",
    "#     http://stackoverflow.com/questions/34407953/how-to-create-dummy-variable-and-then-aggregate-using-scikit-learn#comment61577775_34407953\n",
    "#     http://stackoverflow.com/questions/31661604/efficiently-create-sparse-pivot-tables-in-pandas/31679396#31679396\"\"\"\n",
    "\n",
    "#     frame = DataFrame\n",
    "#     frame.columns = [\"person\",\"thing\"]\n",
    "#     frame = frame.groupby([\"person\", \"thing\"]).size().reset_index().rename(columns={0:'count'})\n",
    "#     print frame\n",
    "\n",
    "#     from scipy.sparse import csr_matrix\n",
    "\n",
    "#     person_u = sorted(list((frame[\"person\"].unique())))\n",
    "#     thing_u = sorted(list((frame[\"thing\"].unique())))\n",
    "\n",
    "#     data = list(frame['count'])\n",
    "#     row = frame.person.astype('category', categories=person_u).cat.codes\n",
    "#     col = frame.thing.astype('category', categories=thing_u).cat.codes\n",
    "#     sparse_matrix = csr_matrix((data, (row, col)), shape=(len(person_u), len(thing_u)))\n",
    "\n",
    "#     sparse_matrix.todense()\n",
    "    \n",
    "#     dfs=pd.SparseDataFrame([ pd.SparseSeries(sparse_matrix[i].toarray().ravel(), fill_value=0) \n",
    "#                               for i in np.arange(sparse_matrix.shape[0]) ], index=person_u, columns=thing_u, default_fill_value=0)\n",
    "\n",
    "#     return dfs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def create_person_item_sparse_matrix(DataFrame):\n",
    "#     \"\"\"When pd.get_dummies fails you... try this.\n",
    "#     DataFrame should have 2 columns: the person & the categorical thing which needs to be dummied.\n",
    "    \n",
    "#     References: See Chen/Affatus back and forth here:\n",
    "#     http://stackoverflow.com/questions/34407953/how-to-create-dummy-variable-and-then-aggregate-using-scikit-learn#comment61577775_34407953\n",
    "#     http://stackoverflow.com/questions/31661604/efficiently-create-sparse-pivot-tables-in-pandas/31679396#31679396\"\"\"\n",
    "\n",
    "#     frame = DataFrame\n",
    "#     frame.columns = [\"person\",\"thing\"]\n",
    "#     frame = frame.groupby([\"person\", \"thing\"]).size().reset_index().rename(columns={0:'count'})\n",
    "#     # print frame\n",
    "\n",
    "#     from scipy.sparse import csr_matrix\n",
    "\n",
    "#     person_u = sorted(list((frame[\"person\"].unique())))\n",
    "#     thing_u = sorted(list((frame[\"thing\"].unique())))\n",
    "\n",
    "#     data = list(frame['count'])\n",
    "#     row = frame.person.astype('category', categories=person_u).cat.codes\n",
    "#     col = frame.thing.astype('category', categories=thing_u).cat.codes\n",
    "#     sparse_matrix = csr_matrix((data, (row, col)), shape=(len(person_u), len(thing_u)))\n",
    "\n",
    "#     sparse_matrix.todense()\n",
    "#     sparse_matrix_series = [pd.SparseSeries(sparse_matrix[i].toarray().ravel(), fill_value=0) for i in np.arange(sparse_matrix.shape[0])]\n",
    "\n",
    "#     return (person_u,thing_u,sparse_matrix_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # #These will probably need to be run independently (not all at once) as they take too long.\n",
    "# # genre_feature_dummied = pd.get_dummies(genre_1_raw).groupby(\"movieID\", as_index = False).sum()\n",
    "# # directors_feature_dummied = pd.get_dummies(directors_1_raw).groupby(\"movieID\", as_index = False).sum()\n",
    "# # tags_feature_dummied = pd.get_dummies(tags_1_raw).groupby(\"movieID\", as_index = False).sum()\n",
    "# # actors_2_as_dummied = create_sparse_dummy_data_frame(actors_1_raw)\n",
    "\n",
    "# genre_feature_dummied.to_pickle(\"../Resources/Pickles/genre_feature_dummied.pickle\")\n",
    "# directors_feature_dummied.to_pickle(\"../Resources/Pickles/directors_feature_dummied.pickle\")\n",
    "# tags_feature_dummied.to_pickle(\"../Resources/Pickles/tags_feature_dummied.pickle\")\n",
    "# #actors_feature_as_dummied.to_pickle(\"../Resources/Pickles/actors_feature_as_dummied.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "genre_feature_dummied = pd.read_pickle(\"../Resources/Pickles/genre_feature_dummied.pickle\")\n",
    "directors_feature_dummied = pd.read_pickle(\"../Resources/Pickles/directors_feature_dummied.pickle\")\n",
    "tags_feature_dummied = pd.read_pickle(\"../Resources/Pickles/tags_feature_dummied.pickle\")\n",
    "#actors_feature_as_dummied = pd.read_pickle(\"../Resources/Pickles/actors_2_as_dummied.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#actors_2_as_dummied = create_sparse_dummy_data_frame(actors_1_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# with open(\"../Resources/Pickles/person_u.pickle\", 'wb') as f:\n",
    "#     pickle.dump(person_u, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with open(\"../Resources/Pickles/thing_u.pickle\", 'wb') as f:\n",
    "#     pickle.dump(thing_u, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "actors_1_raw = dictionary_of_hetrec_dataframes[\"movie_actors\"][[\"movieID\",\"actorID\"]]          # 200 k actors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unique_movies_count = len(actors_1_raw[\"movieID\"].unique()) # 10174\n",
    "unique_actors_count = len(actors_1_raw[\"actorID\"].unique()) # 95321\n",
    "unique_movies_IDs = actors_1_raw[\"movieID\"].unique() # 10174\n",
    "unique_actors_names = actors_1_raw[\"actorID\"].unique() # 95321"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create empty dataframe \n",
    "number_of_movies = unique_movies_count\n",
    "number_of_actors = unique_actors_count\n",
    "actors_2_as_dummied = np.zeros((number_of_movies,number_of_actors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed - 1\n",
      "Completed - 2\n",
      "Completed - 3\n",
      "Completed - 4\n"
     ]
    }
   ],
   "source": [
    "print \"Completed - 1\"\n",
    "# Turn into a dataframe\n",
    "actors_2_as_dummied = pd.DataFrame(actors_2_as_dummied)\n",
    "print \"Completed - 2\"\n",
    "# Adjust indicies \n",
    "actors_2_as_dummied.index = unique_movies_IDs\n",
    "print \"Completed - 3\"\n",
    "# Adjust columns \n",
    "actors_2_as_dummied.columns = unique_actors_names\n",
    "print \"Completed - 4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "231742\n"
     ]
    }
   ],
   "source": [
    "all_actor_movie_combos = len(actors_1_raw)\n",
    "print all_actor_movie_combos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 11000 12000 13000 14000 15000 16000 17000 18000 19000 20000 21000 22000 23000 24000 25000 26000 27000 28000 29000 30000 31000 32000 33000 34000 35000 36000 37000 38000 39000 40000 41000 42000 43000 44000 45000 46000 47000 48000 49000 50000 51000 52000 53000 54000 55000 56000 57000 58000 59000 60000 61000 62000 63000 64000 65000 66000 67000 68000 69000 70000 71000 72000 73000 74000 75000 76000 77000 78000 79000 80000 81000 82000 83000 84000 85000 86000 87000 88000 89000 90000 91000 92000 93000 94000 95000 96000 97000 98000 99000 100000 101000 102000 103000 104000 105000 106000 107000 108000 109000 110000 111000 112000 113000 114000 115000 116000 117000 118000 119000 120000 121000 122000 123000 124000 125000 126000 127000 128000 129000 130000 131000 132000 133000 134000 135000 136000 137000 138000 139000 140000 141000 142000 143000 144000 145000 146000 147000 148000 149000 150000 151000 152000 153000 154000 155000 156000 157000 158000 159000 160000 161000 162000 163000 164000 165000 166000 167000 168000 169000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-aab1659f73a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m#print movie_aka_index, actor_name, column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mactors_2_as_dummied\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmovie_aka_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactor_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/pandas/core/indexing.pyc\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_setitem_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_with_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_has_valid_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/pandas/core/indexing.pyc\u001b[0m in \u001b[0;36m_setitem_with_indexer\u001b[0;34m(self, indexer, value)\u001b[0m\n\u001b[1;32m    565\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m             self.obj._data = self.obj._data.setitem(indexer=indexer,\n\u001b[0;32m--> 567\u001b[0;31m                                                     value=value)\n\u001b[0m\u001b[1;32m    568\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_update_cacher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/pandas/core/internals.pyc\u001b[0m in \u001b[0;36msetitem\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   2915\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2916\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msetitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2917\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'setitem'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2919\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mputmask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/pandas/core/internals.pyc\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, f, axes, filter, do_integrity_check, consolidate, raw, **kwargs)\u001b[0m\n\u001b[1;32m   2888\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2889\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mgr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2890\u001b[0;31m             \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2891\u001b[0m             \u001b[0mresult_blocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_extend_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapplied\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/pandas/core/internals.pyc\u001b[0m in \u001b[0;36msetitem\u001b[0;34m(self, indexer, value, mgr)\u001b[0m\n\u001b[1;32m    719\u001b[0m                 \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 721\u001b[0;31m                 \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_infer_dtype_from_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    722\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m                 \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'infer'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/pandas/core/common.pyc\u001b[0m in \u001b[0;36m_infer_dtype_from_scalar\u001b[0;34m(val)\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'M8[ns]'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimedelta64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimedelta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimedelta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'm8[ns]'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Fill in values:\n",
    "#for i in range(len(actors_1_raw)):\n",
    "for i in range(231742):\n",
    "    if i % 1000 == 0:\n",
    "        print i,\n",
    "    movie_aka_index = actors_1_raw[\"movieID\"][i]\n",
    "    actor_name = actors_1_raw[\"actorID\"][i]\n",
    "\n",
    "    #print movie_aka_index, actor_name, column\n",
    "    actors_2_as_dummied.ix[movie_aka_index, actor_name] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "actors_2_as_dummied = actors_2_as_dummied.reset_index()\n",
    "actors_2_as_dummied.rename(columns={'index':'movieID'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "actors_2_as_dummied.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Unfortunantly these either stop or get really big in size...ergo 10gb... \n",
    "# np.savetxt(\"../Resources/Pickles/actors_2_as_dummied.txt\", actors_2_as_dummied)\n",
    "# actors_2_as_dummied.to_pickle(\"../Resources/Pickles/actors_2_as_dummied.pickle\")\n",
    "# with open(\"../Resources/Pickles/actors_2_as_dummied.pickle\", 'wb') as f:\n",
    "#     pickle.dump(actors_2_as_dummied, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV. Feature Engineering: Running MNF on some particular features..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Saving the movieID columns from reach of the feature data frames\n",
    "genre_feature_movieID_column = genre_feature_dummied[\"movieID\"]\n",
    "directors_feature_movieID_column = directors_feature_dummied[\"movieID\"]\n",
    "tags_feature_movieID_column = tags_feature_dummied[\"movieID\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "actors_feature_movieID_column = actors_2_as_dummied[\"movieID\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Removing the movieID column before running SVD...\n",
    "\n",
    "del genre_feature_dummied[\"movieID\"]\n",
    "del directors_feature_dummied[\"movieID\"]\n",
    "del tags_feature_dummied[\"movieID\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del actors_2_as_dummied[\"movieID\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Getting the total number of  the movieID column before running SVD...\n",
    "\n",
    "genre_dummies_obs_features_tuple = genre_feature_dummied.shape           # (10197, 20)\n",
    "directors_dummies_obs_features_tuple = directors_feature_dummied.shape   # (10155, 4060)\n",
    "tags_dummies_obs_features_tuple = tags_feature_dummied.shape             # (7155, 5297)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "actors_dummies_obs_features_tuple = actors_2_as_dummied.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Running SVD on the big features... \n",
    "# Notes: \n",
    "# - When I was with vinny, this seemed to produce some weird values, so we switched to doing MNF insteand.\n",
    "# - Instead of fussing with getting all the singular values, we just arbitrarily went with 10... This could be chosen better\n",
    "\n",
    "# feature_u_s_vt_dictionary = {}\n",
    "\n",
    "# feature_u_s_vt_dictionary[\"genre_USVt\"] = svds(genre_feature_dummied, k=10, which=\"LM\")\n",
    "# feature_u_s_vt_dictionary[\"directors_USVt\"] = svds(directors_feature_dummied, k=10, which=\"LM\")\n",
    "# feature_u_s_vt_dictionary[\"tags_USVt\"] = svds(tags_feature_dummied, k=10, which=\"LM\")\n",
    "# feature_u_s_vt_dictionary[\"actors_USVt\"] = svds(actors_feature_as_dummied, k=10, which=\"LM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Running MNF on the big features... \n",
    "# Notes:\n",
    "# - Instead of fussing with getting all the singular values, we just arbitrarily went with 10... This could be chosen bette\n",
    "\n",
    "model = NMF(n_components = 10)\n",
    "\n",
    "# Theoretically, \"*****\"_3_postNMF * \"*****\"_3_postNMF_components  is roughly equal to \"*****\"._feature_dummied \n",
    "# (minus the information lost by only taking a limited number of components)\n",
    "\n",
    "# This would look something like this:\n",
    "\n",
    "# a = genre_3_postNMF  \n",
    "# b = genre_3_postNMF_components\n",
    "# c = a.dot(b)\n",
    "# c roughly equal to: genre_2_as_dummies\n",
    "    \n",
    "# \"*****\"_3_postNMF <--- the abbreviated 10 features <--- # This will be the distilled matrix we plug our recomendations\n",
    "# \"*****\"_3_postNMF_components = the conversion matrix which should expand back to the full one\n",
    "\n",
    "# directors_3_postNMF = model.fit_transform(directors_feature_dummied)\n",
    "# directors_3_postNMF_components = model.components_\n",
    "# # print directors_2_as_dummies.shape          # (10155,4060)\n",
    "# # print directors_3_postNMF.shape             # 10155,10)\n",
    "# # print directors_3_postNMF_components.shape  # (10,4060)\n",
    "\n",
    "# genre_3_postNMF = model.fit_transform(genre_feature_dummied)    \n",
    "# genre_3_postNMF_components = model.components_\n",
    "\n",
    "# tags_3_postNMF = model.fit_transform(tags_feature_dummied)\n",
    "# tags_3_postNMF_components = model.components_   \n",
    "\n",
    "actors_3_postNMF = model.fit_transform(actors_2_as_dummied)\n",
    "actors_3_postNMF_components = model.components_   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print actors_3_postNMF_components.shape\n",
    "print actors_3_postNMF.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print type(actors_3_postNMF_components)\n",
    "print type(actors_3_postNMF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"../Resources/Pickles/actors_3_postNMF.pickle\", 'wb') as handle:\n",
    "  pickle.dump(actors_3_postNMF, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"../Resources/Pickles/actors_3_postNMF_components.pickle\", 'wb') as handle:\n",
    "  pickle.dump(actors_3_postNMF_components, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Doing MNF - the special way which accounts for spare matrixies...\n",
    "# http://stackoverflow.com/questions/22767695/python-non-negative-matrix-factorization-that-handles-both-zeros-and-missing-dat\n",
    "# ?? Perhaps consider using a more developed library ?? Nimfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import linalg\n",
    "from numpy import dot\n",
    "\n",
    "def nmf(X, latent_features, max_iter=100, error_limit=1e-6, fit_error_limit=1e-6):\n",
    "    \"\"\"\n",
    "    Decompose X to A*Y\n",
    "    \"\"\"\n",
    "    eps = 1e-5\n",
    "    print 'Starting NMF decomposition with {} latent features and {} iterations.'.format(latent_features, max_iter)\n",
    "    X = X.toarray()  # I am passing in a scipy sparse matrix\n",
    "\n",
    "    # mask\n",
    "    mask = np.sign(X)\n",
    "\n",
    "    # initial matrices. A is random [0,1] and Y is A\\X.\n",
    "    rows, columns = X.shape\n",
    "    A = np.random.rand(rows, latent_features)\n",
    "    A = np.maximum(A, eps)\n",
    "\n",
    "    Y = linalg.lstsq(A, X)[0]\n",
    "    Y = np.maximum(Y, eps)\n",
    "\n",
    "    masked_X = mask * X\n",
    "    X_est_prev = dot(A, Y)\n",
    "    for i in range(1, max_iter + 1):\n",
    "        # ===== updates =====\n",
    "        # Matlab: A=A.*(((W.*X)*Y')./((W.*(A*Y))*Y'));\n",
    "        top = dot(masked_X, Y.T)\n",
    "        bottom = (dot((mask * dot(A, Y)), Y.T)) + eps\n",
    "        A *= top / bottom\n",
    "\n",
    "        A = np.maximum(A, eps)\n",
    "        # print 'A',  np.round(A, 2)\n",
    "\n",
    "        # Matlab: Y=Y.*((A'*(W.*X))./(A'*(W.*(A*Y))));\n",
    "        top = dot(A.T, masked_X)\n",
    "        bottom = dot(A.T, mask * dot(A, Y)) + eps\n",
    "        Y *= top / bottom\n",
    "        Y = np.maximum(Y, eps)\n",
    "        # print 'Y', np.round(Y, 2)\n",
    "\n",
    "\n",
    "        # ==== evaluation ====\n",
    "        if i % 5 == 0 or i == 1 or i == max_iter:\n",
    "            print 'Iteration {}:'.format(i),\n",
    "            X_est = dot(A, Y)\n",
    "            err = mask * (X_est_prev - X_est)\n",
    "            fit_residual = np.sqrt(np.sum(err ** 2))\n",
    "            X_est_prev = X_est\n",
    "\n",
    "            curRes = linalg.norm(mask * (X - X_est), ord='fro')\n",
    "            print 'fit residual', np.round(fit_residual, 4),\n",
    "            print 'total residual', np.round(curRes, 4)\n",
    "            if curRes < error_limit or fit_residual < fit_error_limit:\n",
    "                break\n",
    "\n",
    "    return A, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print genre_feature_dummied.shape\n",
    "print directors_feature_dummied.shape\n",
    "print tags_feature_dummied.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print type(actors_2_as_dummied)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting to sparse matrix....\n",
    "### sA = sparse.csr_matrix(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "genre_3_sparse = sparse.csr_matrix(genre_feature_dummied)\n",
    "print genre_3_sparse.shape\n",
    "print \"genre complete\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "directors_3_sparse = sparse.csr_matrix(directors_feature_dummied)\n",
    "print directors_3_sparse.shape\n",
    "print \"directors complete\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tags_3_sparse = sparse.csr_matrix(tags_feature_dummied)\n",
    "print tags_3_sparse.shape\n",
    "print \"tags complete\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I am waiting for this to finish loading!!! :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "actors_3_sparse = sparse.csr_matrix(actors_2_as_dummied)\n",
    "print \"actors complete\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "genre_3_postNMF_SPECIAL = nmf(genre_3_sparse,10)\n",
    "print genre_3_postNMF_SPECIAL[0].shape, genre_3_postNMF_SPECIAL[1].shape\n",
    "print \"GENRE - COMPLETE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "directors_3_postNMF_SPECIAL = nmf(directors_3_sparse,10)\n",
    "print directors_3_postNMF_SPECIAL[0].shape, directors_3_postNMF_SPECIAL[1].shape\n",
    "print \"Directors - COMPLETE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tags_3_postNMF_SPECIAL = nmf(tags_3_sparse,10)\n",
    "print tags_3_postNMF_SPECIAL[0].shape, tags_3_postNMF_SPECIAL[1].shape\n",
    "print \"Tags - COMPLETE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# [v for v in globals().keys() if not v.startswith('_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "this = sys.modules[__name__]\n",
    "list_of_vars_to_be_removed = ['genre_dummies_obs_features_tuple',\n",
    " 'list_of_urls',\n",
    " 'all_actor_movie_combos',\n",
    " 'dump_svmlight_file',\n",
    " 'user_ratings',\n",
    " 'directors_feature_movieID_column',\n",
    " 'table_path',\n",
    " 'handle',\n",
    " 'list_of_movieIDs',\n",
    " 'actors_dummies_obs_features_tuple',\n",
    " 'unique_movies_count',\n",
    " 'hetrec2011_dat_files',\n",
    " 'tags_3_postNMF_SPECIAL',\n",
    " 'tags_feature_dummied',\n",
    " 'lookup_table_movies',\n",
    " 'tags_headers',\n",
    " 'directors_3_sparse',\n",
    " 'dictionary_of_hetrec_dataframes',\n",
    " 'imdb_and_rt_ratings_feature',\n",
    " 'directors_feature_dim_reduced',\n",
    " 'hetrec2011_dat_files_pwd',\n",
    " 'genre_1_raw',\n",
    " 'actor_name',\n",
    " 'StringIO',\n",
    " 'unique_actors_names',\n",
    " 'Image',\n",
    " 'tags_1_raw',\n",
    " 'tags_feature_dim_reduced',\n",
    " 'tags_dummies_obs_features_tuple',\n",
    " 'genre_feature_dim_reduced',\n",
    " 'hetrec2011_table_names',\n",
    " 'IPython',\n",
    " 'dictionary_of_movie_lens22M_dataframes',\n",
    " 'column',\n",
    " 'misc_movie_features',\n",
    " 'genre_feature_movieID_column',\n",
    " 'number_of_movies',\n",
    " 'linalg',\n",
    " 'genre_feature_dummied',\n",
    " 'directors_feature_dummied',\n",
    " 'directors_dummies_obs_features_tuple',\n",
    " 'genre_headers',\n",
    " 'tags_feature_movieID_column',\n",
    " 'dot',\n",
    " 'directors_1_raw',\n",
    " 'genre_3_postNMF_SPECIAL',\n",
    " 'n',\n",
    " 'movies_df',\n",
    " 'table_name',\n",
    " 'movie_aka_index',\n",
    " 'Out',\n",
    " 'genre_3_sparse',\n",
    " 'number_of_actors',\n",
    " 'svds',\n",
    " 'actors_1_raw',\n",
    " 'director_headers',\n",
    " 'directors_3_postNMF_SPECIAL',\n",
    " 'tags_3_sparse',\n",
    " 'csr_matrix',\n",
    " 'get_ipython',\n",
    " 'unique_movies_IDs',\n",
    " 'np',\n",
    " 'list_of_movieTitles',\n",
    " 'In',\n",
    " 'movie_lens22M_table_names',\n",
    " 'movie_lens22M_csv_files_pwd',\n",
    " 'requests',\n",
    " 'i',\n",
    " 'PIL',\n",
    " 'unique_actors_count',\n",
    " 'movie_lens22M_csv_files']\n",
    "\n",
    "for i in dir():    \n",
    "    if i[0] != \"_\":\n",
    "        if i in list_of_vars_to_be_removed:\n",
    "            print i\n",
    "            delattr(this,str(i))\n",
    "\n",
    "%reset out        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is what I'm waiting for it to load !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# It seems to get chocked up on the last time\n",
    "actors_3_postNMF_SPECIAL = nmf(actors_3_sparse, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "directors_3_postNMF_SPECIAL[1].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# (1) Packaging matrix into DataFrame\n",
    "# (2) Adding Headers to DF \n",
    "# (3) Adding back MovieID Series.\n",
    "\n",
    "################################################################################################ \n",
    "# Directors: Part (1)\n",
    "directors_feature_dim_reduced = pd.DataFrame(directors_3_postNMF_SPECIAL[0])\n",
    "# Directors: Part (2)\n",
    "director_headers = []\n",
    "for i in directors_feature_dim_reduced.columns:\n",
    "    header = \"director\" + \"_nmf_vector_\" + str(i)\n",
    "    director_headers.append(header)\n",
    "directors_feature_dim_reduced.columns = director_headers\n",
    "# Directors: Part (3)\n",
    "directors_feature_dim_reduced[\"movieID\"] = directors_feature_movieID_column\n",
    "################################################################################################\n",
    "# Genre: Part (1)\n",
    "genre_feature_dim_reduced = pd.DataFrame(genre_3_postNMF_SPECIAL[0])\n",
    "# Genre: Part (2)\n",
    "genre_headers = []\n",
    "for i in genre_feature_dim_reduced.columns:\n",
    "    header = \"genre\" + \"_nmf_vector_\" + str(i)\n",
    "    genre_headers.append(header)\n",
    "genre_feature_dim_reduced.columns = genre_headers\n",
    "# Genre: Part (3)\n",
    "genre_feature_dim_reduced[\"movieID\"] = genre_feature_movieID_column\n",
    "################################################################################################\n",
    "# Tags: Part (1)\n",
    "tags_feature_dim_reduced = pd.DataFrame(tags_3_postNMF_SPECIAL[0])\n",
    "# Tags: Part (2)\n",
    "tags_headers = []\n",
    "for i in tags_feature_dim_reduced.columns:\n",
    "    header = \"tags\" + \"_nmf_vector_\" + str(i)\n",
    "    tags_headers.append(header)\n",
    "tags_feature_dim_reduced.columns = tags_headers\n",
    "# Tags: Part (3)\n",
    "tags_feature_dim_reduced[\"movieID\"] = tags_feature_movieID_column\n",
    "\n",
    "################################################################################################\n",
    "\n",
    "# # Actors: Part (1)\n",
    "# actors_feature_dim_reduced = pd.DataFrame(actors_3_postNMF)\n",
    "# # Actors: Part (2)\n",
    "# actors_headers = []\n",
    "# for i in actors_feature_dim_reduced.columns:\n",
    "#     header = \"actors\" + \"nmf_vector_\" + str(i)\n",
    "#     actors_headers.append(header)\n",
    "# actors_feature_dim_reduced.columns = actors_headers\n",
    "# # Actors: Part (3)\n",
    "# actors_feature_dim_reduced[\"movieID\"] = actors_feature_movieID_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# And Now for the pickling....\n",
    "\n",
    "directors_feature_dim_reduced.to_pickle(\"../Resources/Pickles/directors_feature_dim_reduced.pickle\")\n",
    "genre_feature_dim_reduced.to_pickle(\"../Resources/Pickles/genre_feature_dim_reduced.pickle\")\n",
    "tags_feature_dim_reduced.to_pickle(\"../Resources/Pickles/tags_feature_dim_reduced.pickle\")\n",
    "# actors_feature_dim_reduced.to_pickle(\"../Resources/Pickles/actors_feature_dim_reduced.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print directors_feature_dim_reduced.shape\n",
    "directors_feature_dim_reduced.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
